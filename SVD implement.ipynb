{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import init Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy.linalg import pinv\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential \n",
    "from keras.callbacks import TensorBoard,EarlyStopping\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import  seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example : SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X = U . Sigma . V^*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00233226]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# least squares via SVD with pseudoinverse\n",
    "data = array([\n",
    "\t[0.05, 0.12],\n",
    "\t[0.18, 0.22],\n",
    "\t[0.31, 0.35],\n",
    "\t[0.42, 0.38],\n",
    "\t[0.5, 0.49],\n",
    "\t])\n",
    "X, y = data[:,0], data[:,1]\n",
    "X = X.reshape((len(X), 1))\n",
    "# calculate coefficients  # PINV 求解廣義的逆矩陣\n",
    "b = pinv(X).dot(y)\n",
    "print(b)\n",
    "# predict using coefficients\n",
    "yhat = X.dot(b)\n",
    "# plot data and predictions\n",
    "pyplot.scatter(X, y)\n",
    "pyplot.plot(X, yhat, color='red')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabets = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'DESCR', 'feature_names', 'data_filename', 'target_filename'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diabets['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.DataFrame(diabets['data'],columns = diabets['feature_names'])\n",
    "y = pd.DataFrame(diabets['target'],columns = ['disease progression']) # Target为一年后患疾病的定量指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([x,y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(71, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(89, 10)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(282,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(71,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(89,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = data.drop(['disease progression'], axis=1)\n",
    "y = data['disease progression']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 123)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state= 123)\n",
    "\n",
    "display(\n",
    "      X_train.shape,\n",
    "      X_val.shape,\n",
    "      X_test.shape,\n",
    "      y_train.shape,\n",
    "      y_val.shape,\n",
    "      y_test.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Using Keras to build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential, Model as keras_models_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def FlattenWeights(weights):\n",
    "        \"\"\"\n",
    "        flatten weights\n",
    "        \n",
    "        param weights: keras神經網路的權重格式:nparray包在list中\n",
    "        return WeightsStrucure : 神經網路各層的權重shape包在list中，unflatten時會用到\n",
    "        return FlattenedWeights : 一維list包含所有的權重\n",
    "        \"\"\"\n",
    "        WeightsStrucure = []\n",
    "        FlattenedWeights = []\n",
    "        for i_layer in weights:\n",
    "            WeightsStrucure.append(i_layer.shape)\n",
    "            if len(i_layer.shape) == 1 :# 該層權重的shape為一維 e.g. (15,)      \n",
    "                FlattenedWeights.extend(i_layer)\n",
    "            else :# 該層權重的shape為二維 e.g. (30, 15)  \n",
    "                for i_links in i_layer:\n",
    "                    FlattenedWeights.extend(i_links)\n",
    "        return WeightsStrucure, FlattenedWeights\n",
    "\n",
    "def UnflattenWeights(WeightsStrucure, ModifiedWeights):\n",
    "    \"\"\"\n",
    "    Unflatten(回復成原本的結構) weights  \n",
    "\n",
    "    param WeightsStrucure : 神經網路各層的權重shape包在list中\n",
    "    param ModifiedWeights : 一維list包含所有meteHeuristic修改過的權重\n",
    "    return: keras神經網路的權重格式:nparray包在list中\n",
    "    \"\"\"\n",
    "    UnflattenWeights = []\n",
    "    i_index = 0 \n",
    "    for i_layer in WeightsStrucure:\n",
    "        if len(i_layer) == 1 : # 該層權重的shape為一維 e.g. (15,)      \n",
    "            TempList = ModifiedWeights[i_index:(i_index + i_layer[0])]\n",
    "            TempList = np.asarray(TempList)\n",
    "            i_index = i_index + i_layer[0]\n",
    "        else : # 該層權重的shape為二維 e.g. (30, 15)  \n",
    "            TempList = ModifiedWeights[i_index:(i_index + (i_layer[0]*i_layer[1]))]\n",
    "            TempList = np.reshape(TempList, i_layer )\n",
    "            i_index = i_index + (i_layer[0]*i_layer[1])\n",
    "        UnflattenWeights.append(TempList)\n",
    "    return UnflattenWeights    \n",
    "def ModelCompile():\n",
    "    K.clear_session() \n",
    "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "    sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "    tf.keras.backend.set_session(sess)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=10, kernel_initializer='normal', activation='relu',name = 'IntermediateLayer'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def ModelFit(model, weights, epochs):\n",
    "    #class_weight = {0: 1., 1: 1525/2632}\n",
    "    model.set_weights(weights)\n",
    "    modelcallbacks = model.fit(X_train, y_train, epochs=epochs, batch_size=32, verbose=1,\n",
    "        validation_data = (X_val, y_val),\n",
    "        callbacks=[EarlyStopping(monitor='val_loss', patience=3, verbose=2, restore_best_weights=True)],\n",
    "        shuffle=True) #, class_weight=class_weight\n",
    "#     SummarizeHistory(modelcallbacks, UseValid=True)                                        \n",
    "    weights = model.get_weights() \n",
    "    return model, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Build init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "IntermediateLayer (Dense)    (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 282 samples, validate on 71 samples\n",
      "Epoch 1/150\n",
      "282/282 [==============================] - 0s 463us/step - loss: 28557.8469 - val_loss: 31097.8088\n",
      "Epoch 2/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28554.5568 - val_loss: 31094.1571\n",
      "Epoch 3/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28551.0244 - val_loss: 31090.1178\n",
      "Epoch 4/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28547.0370 - val_loss: 31085.6981\n",
      "Epoch 5/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 28542.7233 - val_loss: 31080.8131\n",
      "Epoch 6/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28537.9242 - val_loss: 31075.4287\n",
      "Epoch 7/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28532.6478 - val_loss: 31069.5298\n",
      "Epoch 8/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28526.8741 - val_loss: 31063.0722\n",
      "Epoch 9/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28520.6243 - val_loss: 31056.0212\n",
      "Epoch 10/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 28513.9148 - val_loss: 31048.3347\n",
      "Epoch 11/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28506.5198 - val_loss: 31040.1066\n",
      "Epoch 12/150\n",
      "282/282 [==============================] - 0s 78us/step - loss: 28498.5667 - val_loss: 31031.0197\n",
      "Epoch 13/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28489.8064 - val_loss: 31020.9974\n",
      "Epoch 14/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28480.1830 - val_loss: 31010.0259\n",
      "Epoch 15/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28469.6084 - val_loss: 30998.1784\n",
      "Epoch 16/150\n",
      "282/282 [==============================] - 0s 74us/step - loss: 28458.2154 - val_loss: 30985.3050\n",
      "Epoch 17/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28445.8124 - val_loss: 30971.1627\n",
      "Epoch 18/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 28432.1281 - val_loss: 30955.7985\n",
      "Epoch 19/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28417.3354 - val_loss: 30939.1392\n",
      "Epoch 20/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28401.5616 - val_loss: 30921.1096\n",
      "Epoch 21/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28384.5985 - val_loss: 30901.9754\n",
      "Epoch 22/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 28366.3058 - val_loss: 30881.9680\n",
      "Epoch 23/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28347.1954 - val_loss: 30860.8947\n",
      "Epoch 24/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28327.4781 - val_loss: 30838.5499\n",
      "Epoch 25/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28306.3556 - val_loss: 30815.3539\n",
      "Epoch 26/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28284.1019 - val_loss: 30791.4085\n",
      "Epoch 27/150\n",
      "282/282 [==============================] - 0s 81us/step - loss: 28261.6695 - val_loss: 30766.0330\n",
      "Epoch 28/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 28237.8976 - val_loss: 30739.8445\n",
      "Epoch 29/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 28213.4727 - val_loss: 30712.5734\n",
      "Epoch 30/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28187.9545 - val_loss: 30684.5474\n",
      "Epoch 31/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28161.4944 - val_loss: 30655.8551\n",
      "Epoch 32/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28134.4379 - val_loss: 30626.1894\n",
      "Epoch 33/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28106.8605 - val_loss: 30595.3132\n",
      "Epoch 34/150\n",
      "282/282 [==============================] - 0s 74us/step - loss: 28077.7378 - val_loss: 30563.9719\n",
      "Epoch 35/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28048.2126 - val_loss: 30531.7261\n",
      "Epoch 36/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 28018.6157 - val_loss: 30497.9078\n",
      "Epoch 37/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 27986.6335 - val_loss: 30463.7595\n",
      "Epoch 38/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 27954.6035 - val_loss: 30427.5443\n",
      "Epoch 39/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 27920.3703 - val_loss: 30389.8440\n",
      "Epoch 40/150\n",
      "282/282 [==============================] - 0s 83us/step - loss: 27884.6587 - val_loss: 30350.7488\n",
      "Epoch 41/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 27847.8833 - val_loss: 30310.0452\n",
      "Epoch 42/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 27809.8967 - val_loss: 30267.7295\n",
      "Epoch 43/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 27769.5047 - val_loss: 30224.9128\n",
      "Epoch 44/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 27729.9759 - val_loss: 30179.7792\n",
      "Epoch 45/150\n",
      "282/282 [==============================] - 0s 57us/step - loss: 27687.5141 - val_loss: 30134.1082\n",
      "Epoch 46/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 27645.1720 - val_loss: 30086.9526\n",
      "Epoch 47/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 27601.3562 - val_loss: 30038.7307\n",
      "Epoch 48/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 27555.7764 - val_loss: 29990.3353\n",
      "Epoch 49/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 27510.7706 - val_loss: 29940.4038\n",
      "Epoch 50/150\n",
      "282/282 [==============================] - 0s 74us/step - loss: 27464.2896 - val_loss: 29889.4160\n",
      "Epoch 51/150\n",
      "282/282 [==============================] - 0s 78us/step - loss: 27416.6868 - val_loss: 29837.5889\n",
      "Epoch 52/150\n",
      "282/282 [==============================] - 0s 78us/step - loss: 27368.3636 - val_loss: 29784.8775\n",
      "Epoch 53/150\n",
      "282/282 [==============================] - 0s 74us/step - loss: 27319.6277 - val_loss: 29730.8461\n",
      "Epoch 54/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 27269.3502 - val_loss: 29676.2452\n",
      "Epoch 55/150\n",
      "282/282 [==============================] - 0s 81us/step - loss: 27218.7827 - val_loss: 29620.6358\n",
      "Epoch 56/150\n",
      "282/282 [==============================] - 0s 74us/step - loss: 27167.6887 - val_loss: 29563.8035\n",
      "Epoch 57/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 27114.6850 - val_loss: 29507.0000\n",
      "Epoch 58/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 27062.1022 - val_loss: 29448.9728\n",
      "Epoch 59/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 27007.0908 - val_loss: 29391.4104\n",
      "Epoch 60/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26953.9367 - val_loss: 29331.2501\n",
      "Epoch 61/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 26897.9549 - val_loss: 29271.0231\n",
      "Epoch 62/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26841.8738 - val_loss: 29209.8812\n",
      "Epoch 63/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26785.8905 - val_loss: 29147.5054\n",
      "Epoch 64/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26727.4921 - val_loss: 29085.1118\n",
      "Epoch 65/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 26669.5314 - val_loss: 29021.4462\n",
      "Epoch 66/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 26609.7419 - val_loss: 28957.9423\n",
      "Epoch 67/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 26551.2376 - val_loss: 28892.2124\n",
      "Epoch 68/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 26490.0668 - val_loss: 28826.5605\n",
      "Epoch 69/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 67us/step - loss: 26430.2428 - val_loss: 28759.1918\n",
      "Epoch 70/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 26367.6849 - val_loss: 28692.0217\n",
      "Epoch 71/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26304.5031 - val_loss: 28625.0735\n",
      "Epoch 72/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26242.7336 - val_loss: 28556.0309\n",
      "Epoch 73/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26178.2738 - val_loss: 28487.1366\n",
      "Epoch 74/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 26113.5272 - val_loss: 28417.8329\n",
      "Epoch 75/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26049.9670 - val_loss: 28346.3901\n",
      "Epoch 76/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 25983.8260 - val_loss: 28275.2508\n",
      "Epoch 77/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 25918.0534 - val_loss: 28203.1215\n",
      "Epoch 78/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 25850.6250 - val_loss: 28131.2622\n",
      "Epoch 79/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 25784.3958 - val_loss: 28057.4016\n",
      "Epoch 80/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 25715.9218 - val_loss: 27983.7843\n",
      "Epoch 81/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 25648.4689 - val_loss: 27908.7438\n",
      "Epoch 82/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 25579.3979 - val_loss: 27833.4376\n",
      "Epoch 83/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 25509.2791 - val_loss: 27758.0401\n",
      "Epoch 84/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 25438.6166 - val_loss: 27682.4769\n",
      "Epoch 85/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 25368.4253 - val_loss: 27605.9599\n",
      "Epoch 86/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 25298.0272 - val_loss: 27528.4532\n",
      "Epoch 87/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 25227.2403 - val_loss: 27449.6348\n",
      "Epoch 88/150\n",
      "282/282 [==============================] - 0s 78us/step - loss: 25153.3973 - val_loss: 27371.9293\n",
      "Epoch 89/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 25082.0304 - val_loss: 27292.3365\n",
      "Epoch 90/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 25007.8953 - val_loss: 27213.2657\n",
      "Epoch 91/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 24935.1013 - val_loss: 27132.7164\n",
      "Epoch 92/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24860.9654 - val_loss: 27051.4889\n",
      "Epoch 93/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24786.6710 - val_loss: 26969.3661\n",
      "Epoch 94/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 24710.0992 - val_loss: 26888.3294\n",
      "Epoch 95/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24635.2692 - val_loss: 26806.2387\n",
      "Epoch 96/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 24558.4499 - val_loss: 26724.0422\n",
      "Epoch 97/150\n",
      "282/282 [==============================] - 0s 78us/step - loss: 24482.1951 - val_loss: 26640.9551\n",
      "Epoch 98/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24405.3529 - val_loss: 26556.8900\n",
      "Epoch 99/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24328.0466 - val_loss: 26472.2165\n",
      "Epoch 100/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 24250.7240 - val_loss: 26386.7230\n",
      "Epoch 101/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24170.5721 - val_loss: 26302.4284\n",
      "Epoch 102/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24093.1119 - val_loss: 26216.1084\n",
      "Epoch 103/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24014.1228 - val_loss: 26129.2171\n",
      "Epoch 104/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 23934.1254 - val_loss: 26042.5371\n",
      "Epoch 105/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 23854.4594 - val_loss: 25955.4905\n",
      "Epoch 106/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 23773.3237 - val_loss: 25869.2126\n",
      "Epoch 107/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 23693.7227 - val_loss: 25781.4416\n",
      "Epoch 108/150\n",
      "282/282 [==============================] - 0s 78us/step - loss: 23611.9685 - val_loss: 25694.1753\n",
      "Epoch 109/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 23531.5250 - val_loss: 25605.8630\n",
      "Epoch 110/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 23450.0868 - val_loss: 25516.7139\n",
      "Epoch 111/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 23367.5800 - val_loss: 25427.5378\n",
      "Epoch 112/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 23285.1931 - val_loss: 25337.7205\n",
      "Epoch 113/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 23202.2575 - val_loss: 25247.6280\n",
      "Epoch 114/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 23120.1190 - val_loss: 25156.3337\n",
      "Epoch 115/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 23035.0486 - val_loss: 25066.7260\n",
      "Epoch 116/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 22953.2105 - val_loss: 24974.4914\n",
      "Epoch 117/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 22868.5325 - val_loss: 24882.5815\n",
      "Epoch 118/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 22781.0547 - val_loss: 24792.9770\n",
      "Epoch 119/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 22699.6028 - val_loss: 24699.4908\n",
      "Epoch 120/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 22613.8990 - val_loss: 24606.3952\n",
      "Epoch 121/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 22528.7584 - val_loss: 24512.9313\n",
      "Epoch 122/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 22443.4432 - val_loss: 24419.3350\n",
      "Epoch 123/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 22355.3420 - val_loss: 24327.9331\n",
      "Epoch 124/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 22270.6088 - val_loss: 24234.5550\n",
      "Epoch 125/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 22186.7226 - val_loss: 24138.2949\n",
      "Epoch 126/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 22097.9860 - val_loss: 24044.1646\n",
      "Epoch 127/150\n",
      "282/282 [==============================] - 0s 74us/step - loss: 22011.1021 - val_loss: 23949.7518\n",
      "Epoch 128/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 21924.5928 - val_loss: 23854.6995\n",
      "Epoch 129/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 21836.2188 - val_loss: 23760.5921\n",
      "Epoch 130/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 21751.9124 - val_loss: 23663.7549\n",
      "Epoch 131/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 21660.8796 - val_loss: 23570.2630\n",
      "Epoch 132/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 21575.1703 - val_loss: 23473.8694\n",
      "Epoch 133/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 21485.3015 - val_loss: 23379.2471\n",
      "Epoch 134/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 21399.4059 - val_loss: 23282.3041\n",
      "Epoch 135/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 21309.5122 - val_loss: 23186.4036\n",
      "Epoch 136/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 21220.3887 - val_loss: 23090.8009\n",
      "Epoch 137/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 21133.4416 - val_loss: 22992.9347\n",
      "Epoch 138/150\n",
      "282/282 [==============================] - 0s 78us/step - loss: 21044.1592 - val_loss: 22895.3437\n",
      "Epoch 139/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 20953.9622 - val_loss: 22798.6762\n",
      "Epoch 140/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 20864.5969 - val_loss: 22701.6633\n",
      "Epoch 141/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 20775.4453 - val_loss: 22604.3126\n",
      "Epoch 142/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 67us/step - loss: 20685.4906 - val_loss: 22507.0308\n",
      "Epoch 143/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 20596.4577 - val_loss: 22408.3209\n",
      "Epoch 144/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 20504.6767 - val_loss: 22311.2393\n",
      "Epoch 145/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 20415.8431 - val_loss: 22212.4951\n",
      "Epoch 146/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 20323.2588 - val_loss: 22116.0328\n",
      "Epoch 147/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 20237.2169 - val_loss: 22014.8780\n",
      "Epoch 148/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 20143.8119 - val_loss: 21916.7949\n",
      "Epoch 149/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 20053.2746 - val_loss: 21818.6528\n",
      "Epoch 150/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 19964.4797 - val_loss: 21718.6268\n"
     ]
    }
   ],
   "source": [
    "model = ModelCompile()\n",
    "weights = model.get_weights() \n",
    "model, weights = ModelFit(model=model, weights=weights, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19911.485629880808"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model.predict(X_train)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_train, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.2 Using Linear Reg to minimize error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_name = 'IntermediateLayer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure, flat_weight = FlattenWeights(weights)\n",
    "UnflattenedWeights = UnflattenWeights(structure, flat_weight )\n",
    "model.set_weights(UnflattenedWeights)\n",
    "intermediate_layer_model = keras_models_Model(inputs=model.input,\n",
    "                                              outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(X_train)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression().fit(intermediate_output, y_train)\n",
    "\n",
    "#get OutLayerWeights\n",
    "OutLayerWeights = [np.array(lm.coef_).reshape(structure[-2]),\n",
    "                   np.array(lm.intercept_).reshape(structure[-1])]\n",
    "\n",
    "#update ES-optimized weights\n",
    "UnflattenedWeights[-2:] = OutLayerWeights        \n",
    "\n",
    "model.set_weights(UnflattenedWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3077.8420727330704"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model.predict(X_train)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_train, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Use SVD implement reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Build init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "IntermediateLayer (Dense)    (None, 10)                110       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 282 samples, validate on 71 samples\n",
      "Epoch 1/150\n",
      "282/282 [==============================] - 0s 453us/step - loss: 28557.6877 - val_loss: 31097.5228\n",
      "Epoch 2/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28554.0464 - val_loss: 31093.3045\n",
      "Epoch 3/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28549.8590 - val_loss: 31088.5069\n",
      "Epoch 4/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28545.1677 - val_loss: 31083.0814\n",
      "Epoch 5/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28539.7780 - val_loss: 31077.0138\n",
      "Epoch 6/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28533.8129 - val_loss: 31070.1753\n",
      "Epoch 7/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28527.1402 - val_loss: 31062.5318\n",
      "Epoch 8/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28519.8220 - val_loss: 31054.0142\n",
      "Epoch 9/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28511.4745 - val_loss: 31044.7869\n",
      "Epoch 10/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28502.5601 - val_loss: 31034.6334\n",
      "Epoch 11/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 28492.6866 - val_loss: 31023.6593\n",
      "Epoch 12/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 28482.0915 - val_loss: 31011.7680\n",
      "Epoch 13/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28470.7112 - val_loss: 30998.9171\n",
      "Epoch 14/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28458.5856 - val_loss: 30985.0939\n",
      "Epoch 15/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28445.4570 - val_loss: 30970.4369\n",
      "Epoch 16/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28431.3796 - val_loss: 30955.0449\n",
      "Epoch 17/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28416.8516 - val_loss: 30938.5488\n",
      "Epoch 18/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28401.2688 - val_loss: 30921.1991\n",
      "Epoch 19/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28384.7819 - val_loss: 30903.0629\n",
      "Epoch 20/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28367.6731 - val_loss: 30883.9594\n",
      "Epoch 21/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 28349.7021 - val_loss: 30863.9207\n",
      "Epoch 22/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28330.6668 - val_loss: 30843.1828\n",
      "Epoch 23/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28310.8348 - val_loss: 30821.6412\n",
      "Epoch 24/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 28290.7703 - val_loss: 30798.7394\n",
      "Epoch 25/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28269.4111 - val_loss: 30775.0212\n",
      "Epoch 26/150\n",
      "282/282 [==============================] - 0s 74us/step - loss: 28247.1773 - val_loss: 30750.5897\n",
      "Epoch 27/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28224.0940 - val_loss: 30725.5863\n",
      "Epoch 28/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 28200.3485 - val_loss: 30699.6764\n",
      "Epoch 29/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28175.8566 - val_loss: 30672.8135\n",
      "Epoch 30/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28150.5838 - val_loss: 30644.9544\n",
      "Epoch 31/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28124.4808 - val_loss: 30616.2220\n",
      "Epoch 32/150\n",
      "282/282 [==============================] - 0s 74us/step - loss: 28097.4778 - val_loss: 30586.6686\n",
      "Epoch 33/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 28070.0248 - val_loss: 30555.9966\n",
      "Epoch 34/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28041.6155 - val_loss: 30524.4942\n",
      "Epoch 35/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 28012.0354 - val_loss: 30492.5028\n",
      "Epoch 36/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 27981.9517 - val_loss: 30459.7104\n",
      "Epoch 37/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 27951.1655 - val_loss: 30426.1730\n",
      "Epoch 38/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 27919.7433 - val_loss: 30391.6158\n",
      "Epoch 39/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 27887.3903 - val_loss: 30356.2396\n",
      "Epoch 40/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 27854.4639 - val_loss: 30319.9884\n",
      "Epoch 41/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 27820.9769 - val_loss: 30282.6388\n",
      "Epoch 42/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 27785.7619 - val_loss: 30245.1823\n",
      "Epoch 43/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 27750.6275 - val_loss: 30206.8250\n",
      "Epoch 44/150\n",
      "282/282 [==============================] - 0s 78us/step - loss: 27714.7664 - val_loss: 30167.4124\n",
      "Epoch 45/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 27678.3556 - val_loss: 30126.8982\n",
      "Epoch 46/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 27640.2833 - val_loss: 30086.2440\n",
      "Epoch 47/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 27602.0902 - val_loss: 30044.8410\n",
      "Epoch 48/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 27563.5481 - val_loss: 30002.2910\n",
      "Epoch 49/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 27524.4367 - val_loss: 29958.6212\n",
      "Epoch 50/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 27484.5008 - val_loss: 29914.0590\n",
      "Epoch 51/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 27442.7575 - val_loss: 29869.7373\n",
      "Epoch 52/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 27401.2660 - val_loss: 29824.5266\n",
      "Epoch 53/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 27358.4956 - val_loss: 29779.0535\n",
      "Epoch 54/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 27316.2265 - val_loss: 29732.1840\n",
      "Epoch 55/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 27272.5599 - val_loss: 29684.4111\n",
      "Epoch 56/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 27228.4846 - val_loss: 29635.7641\n",
      "Epoch 57/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 27182.9910 - val_loss: 29587.0192\n",
      "Epoch 58/150\n",
      "282/282 [==============================] - 0s 70us/step - loss: 27138.1350 - val_loss: 29536.7052\n",
      "Epoch 59/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 27091.2983 - val_loss: 29486.2233\n",
      "Epoch 60/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 27045.1069 - val_loss: 29434.6757\n",
      "Epoch 61/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26997.3105 - val_loss: 29382.8750\n",
      "Epoch 62/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26949.2884 - val_loss: 29330.4551\n",
      "Epoch 63/150\n",
      "282/282 [==============================] - 0s 62us/step - loss: 26900.3382 - val_loss: 29277.6708\n",
      "Epoch 64/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 26851.2131 - val_loss: 29224.1293\n",
      "Epoch 65/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26801.3303 - val_loss: 29170.0117\n",
      "Epoch 66/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26750.4581 - val_loss: 29115.5065\n",
      "Epoch 67/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 26699.7880 - val_loss: 29059.7699\n",
      "Epoch 68/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 26648.7586 - val_loss: 29002.8751\n",
      "Epoch 69/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 71us/step - loss: 26594.8848 - val_loss: 28946.9692\n",
      "Epoch 70/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 26543.3001 - val_loss: 28888.7843\n",
      "Epoch 71/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 26489.4501 - val_loss: 28830.3468\n",
      "Epoch 72/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26434.6043 - val_loss: 28771.9469\n",
      "Epoch 73/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26380.9890 - val_loss: 28711.8840\n",
      "Epoch 74/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26324.7972 - val_loss: 28652.1169\n",
      "Epoch 75/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26269.2832 - val_loss: 28591.4620\n",
      "Epoch 76/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 26212.8580 - val_loss: 28530.3071\n",
      "Epoch 77/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 26156.7186 - val_loss: 28467.8075\n",
      "Epoch 78/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 26098.8783 - val_loss: 28405.2098\n",
      "Epoch 79/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 26040.9341 - val_loss: 28342.1264\n",
      "Epoch 80/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 25982.5716 - val_loss: 28278.5715\n",
      "Epoch 81/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 25923.6705 - val_loss: 28214.4103\n",
      "Epoch 82/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 25863.6102 - val_loss: 28150.3368\n",
      "Epoch 83/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 25804.7375 - val_loss: 28084.3111\n",
      "Epoch 84/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 25743.4561 - val_loss: 28018.4920\n",
      "Epoch 85/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 25681.9513 - val_loss: 27952.6108\n",
      "Epoch 86/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 25621.2685 - val_loss: 27885.2565\n",
      "Epoch 87/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 25559.2622 - val_loss: 27817.4699\n",
      "Epoch 88/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 25497.1188 - val_loss: 27749.1236\n",
      "Epoch 89/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 25433.5580 - val_loss: 27681.0581\n",
      "Epoch 90/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 25370.6332 - val_loss: 27612.1892\n",
      "Epoch 91/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 25306.7329 - val_loss: 27543.0028\n",
      "Epoch 92/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 25242.4719 - val_loss: 27473.2195\n",
      "Epoch 93/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 25177.7545 - val_loss: 27402.7308\n",
      "Epoch 94/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 25112.3118 - val_loss: 27331.9553\n",
      "Epoch 95/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 25047.1107 - val_loss: 27260.0379\n",
      "Epoch 96/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24980.0756 - val_loss: 27188.6995\n",
      "Epoch 97/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24913.4995 - val_loss: 27116.9537\n",
      "Epoch 98/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 24848.1162 - val_loss: 27043.0398\n",
      "Epoch 99/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 24780.1519 - val_loss: 26969.3772\n",
      "Epoch 100/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24712.5421 - val_loss: 26894.7421\n",
      "Epoch 101/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 24643.1845 - val_loss: 26821.1744\n",
      "Epoch 102/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 24574.6713 - val_loss: 26747.0785\n",
      "Epoch 103/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24506.2787 - val_loss: 26671.8581\n",
      "Epoch 104/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24436.6837 - val_loss: 26596.6014\n",
      "Epoch 105/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 24367.6267 - val_loss: 26520.2535\n",
      "Epoch 106/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24297.2132 - val_loss: 26443.8175\n",
      "Epoch 107/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24225.8928 - val_loss: 26367.7664\n",
      "Epoch 108/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24156.2064 - val_loss: 26290.3134\n",
      "Epoch 109/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 24085.0491 - val_loss: 26212.3995\n",
      "Epoch 110/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 24012.6674 - val_loss: 26135.1233\n",
      "Epoch 111/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 23940.7526 - val_loss: 26057.4425\n",
      "Epoch 112/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 23869.4387 - val_loss: 25978.5294\n",
      "Epoch 113/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 23796.2671 - val_loss: 25899.9167\n",
      "Epoch 114/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 23723.5653 - val_loss: 25820.7037\n",
      "Epoch 115/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 23649.9556 - val_loss: 25741.3060\n",
      "Epoch 116/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 23577.7640 - val_loss: 25660.4509\n",
      "Epoch 117/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 23502.9294 - val_loss: 25580.1566\n",
      "Epoch 118/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 23428.7436 - val_loss: 25499.6040\n",
      "Epoch 119/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 23354.0219 - val_loss: 25418.8576\n",
      "Epoch 120/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 23279.5505 - val_loss: 25337.4672\n",
      "Epoch 121/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 23204.7628 - val_loss: 25255.5431\n",
      "Epoch 122/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 23129.4725 - val_loss: 25173.2573\n",
      "Epoch 123/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 23054.0488 - val_loss: 25090.8001\n",
      "Epoch 124/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 22977.5074 - val_loss: 25008.6212\n",
      "Epoch 125/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 22901.0490 - val_loss: 24926.2753\n",
      "Epoch 126/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 22826.1006 - val_loss: 24842.0409\n",
      "Epoch 127/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 22748.2054 - val_loss: 24758.7038\n",
      "Epoch 128/150\n",
      "282/282 [==============================] - 0s 71us/step - loss: 22672.0386 - val_loss: 24674.5162\n",
      "Epoch 129/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 22593.6886 - val_loss: 24591.3802\n",
      "Epoch 130/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 22517.4574 - val_loss: 24506.8702\n",
      "Epoch 131/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 22440.5458 - val_loss: 24421.4525\n",
      "Epoch 132/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 22361.1860 - val_loss: 24337.5839\n",
      "Epoch 133/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 22282.8696 - val_loss: 24253.5963\n",
      "Epoch 134/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 22206.2445 - val_loss: 24167.7896\n",
      "Epoch 135/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 22127.6427 - val_loss: 24081.8809\n",
      "Epoch 136/150\n",
      "282/282 [==============================] - 0s 63us/step - loss: 22048.2170 - val_loss: 23996.5228\n",
      "Epoch 137/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 21968.9838 - val_loss: 23910.9890\n",
      "Epoch 138/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 21889.3773 - val_loss: 23825.6505\n",
      "Epoch 139/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 21811.3960 - val_loss: 23738.4104\n",
      "Epoch 140/150\n",
      "282/282 [==============================] - 0s 67us/step - loss: 21730.5873 - val_loss: 23652.2447\n",
      "Epoch 141/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 21651.9766 - val_loss: 23564.5366\n",
      "Epoch 142/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 64us/step - loss: 21570.8666 - val_loss: 23478.0176\n",
      "Epoch 143/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 21491.5143 - val_loss: 23390.9664\n",
      "Epoch 144/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 21411.7158 - val_loss: 23303.1773\n",
      "Epoch 145/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 21330.7730 - val_loss: 23215.7073\n",
      "Epoch 146/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 21250.6548 - val_loss: 23127.5716\n",
      "Epoch 147/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 21169.7121 - val_loss: 23039.6424\n",
      "Epoch 148/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 21088.2748 - val_loss: 22952.0281\n",
      "Epoch 149/150\n",
      "282/282 [==============================] - 0s 64us/step - loss: 21007.6291 - val_loss: 22863.9912\n",
      "Epoch 150/150\n",
      "282/282 [==============================] - 0s 60us/step - loss: 20926.7363 - val_loss: 22775.1602\n"
     ]
    }
   ],
   "source": [
    "model = ModelCompile()\n",
    "weights = model.get_weights() \n",
    "model, weights = ModelFit(model=model, weights=weights, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20880.279718377893"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model.predict(X_train)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_train, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 get intermediate layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure, flat_weight = FlattenWeights(weights)\n",
    "# UnflattenedWeights = UnflattenWeights(structure, flat_weight)\n",
    "model.set_weights(weights)\n",
    "intermediate_layer_model = keras_models_Model(inputs=model.input,\n",
    "                                              outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.42845354e+03 -4.68229802e-12 -5.11380062e+02  1.18202121e+03\n",
      " -2.77329467e+03 -1.15271089e+03  1.71856210e+03  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "reg_input = intermediate_output.reshape((len(intermediate_output), 10))\n",
    "# calculate coefficients  # PINV 求解廣義的逆矩陣\n",
    "b = pinv(reg_input).dot(y_train)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = reg_input.dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3444.7100015676383"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_train, yhat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
